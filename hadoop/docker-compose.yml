version: '2.1'

services:
  namenode:
    image: urykhy/hadoop-namenode
    container_name: namenode
    hostname: namenode
    networks:
      - hadoop
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    environment:
      CLUSTER_NAME: uh-cluster
    healthcheck:
      test: curl 127.0.0.1:8020
      interval: 5s
      timeout: 5s
      retries: 10
    expose:
      - "8020"
      - "9870"

  datanode1:
    image: urykhy/hadoop
    container_name: datanode1
    hostname: datanode1.hadoop
    command: bash -c "yarn nodemanager & exec hdfs datanode"
    depends_on:
      namenode:
        condition: service_healthy
    networks:
      - hadoop
    volumes:
      - hadoop_datanode1:/hadoop/dfs/data
    healthcheck:
      test: test `curl http://datanode1:9864/jmx | grep NamenodeAddresses | wc -l` = '1'
      interval: 5s
      timeout: 5s
      retries: 10
    expose:
      - "8042"
      - "9864"
      - "9866"
      - "9867"

  datanode2:
    image: urykhy/hadoop
    container_name: datanode2
    hostname: datanode2.hadoop
    command: bash -c "yarn nodemanager & exec hdfs datanode"
    depends_on:
      namenode:
        condition: service_healthy
    networks:
      - hadoop
    volumes:
      - hadoop_datanode2:/hadoop/dfs/data
    healthcheck:
      test: test `curl http://datanode2:9864/jmx | grep NamenodeAddresses | wc -l` = '1'
      interval: 5s
      timeout: 5s
      retries: 10
    expose:
      - "8042"
      - "9864"
      - "9866"
      - "9867"

  resourcemanager:
    image: urykhy/hadoop
    container_name: resourcemanager
    hostname: resourcemanager
    command: yarn resourcemanager
    depends_on:
      datanode1:
        condition: service_healthy
      datanode2:
        condition: service_healthy
    networks:
      - hadoop
    expose:
      - "8088"

  historyserver:
    image: urykhy/hadoop
    container_name: historyserver
    hostname: historyserver
    command: yarn timelineserver
    depends_on:
      datanode1:
        condition: service_healthy
      datanode2:
        condition: service_healthy
    networks:
      - hadoop
    volumes:
      - hadoop_historyserver:/hadoop/yarn/timeline
    expose:
      - "8188"

  nfs:
    image: urykhy/hadoop
    container_name: nfs
    hostname: nfs
    command: bash -c "hdfs portmap & exec hdfs nfs3"
    depends_on:
      datanode1:
        condition: service_healthy
      datanode2:
        condition: service_healthy
    networks:
    - hadoop
    expose:
      - "111"
      - "50079"

  minio:
    image: minio/minio
    container_name: minio_hdfs
    hostname: minio
    depends_on:
      datanode1:
        condition: service_healthy
      datanode2:
        condition: service_healthy
    networks:
    - hadoop
    environment:
      MINIO_ACCESS_KEY: minio
      MINIO_SECRET_KEY: minio123
    command: gateway hdfs hdfs://namenode:8020
    volumes:
      - hadoop_minio:/data
    expose:
      - "9000"

        #  spark:
        #    image: urykhy/hadoop-spark
        #    container_name: spark
        #    hostname: spark
        #    networks:
        #      - hadoop
        #    environment:
        #      - SPARK_CONF_spark_eventLog_enabled=true
        #      - SPARK_CONF_spark_eventLog_dir=hdfs://namenode.hadoop:8020/spark-logs
        #      - SPARK_CONF_spark_history_fs_logDirectory=hdfs://namenode.hadoop:8020/spark-logs
        #    env_file:
        #      - ./hadoop.env
        #    command: historyserver
        #    depends_on:
        #      - namenode
        #
        #  notebook:
        #    image: urykhy/hadoop-spark-notebook
        #    container_name: notebook
        #    hostname: notebook
        #    networks:
        #      - hadoop
        #    env_file:
        #      - ./hadoop.env
        #    depends_on:
        #      - spark

networks:
  hadoop:
    external: true

volumes:
  hadoop_namenode:
    external: true
  hadoop_datanode1:
    external: true
  hadoop_datanode2:
    external: true
  hadoop_historyserver:
    external: true
  hadoop_minio:
    external: true

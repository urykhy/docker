version: '3.9'

services:
  kms:
    image: urykhy/hadoop
    container_name: kms
    hostname: kms
    domainname: hadoop.docker
    command: hadoop kms
    networks:
      - hadoop
    volumes:
      - hadoop_kms:/hadoop
    expose:
      - "9600"
    healthcheck:
      test: jps | grep -q KMSWebServer
      interval: 5s
      timeout: 5s
      retries: 10

  namenode:
    image: urykhy/hadoop
    container_name: namenode
    hostname: namenode
    domainname: hadoop.docker
    command: /namenode.sh
    networks:
      - hadoop
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    environment:
      CLUSTER_NAME: uh-cluster
    healthcheck:
      test: curl 127.0.0.1:8020
      interval: 5s
      timeout: 5s
      retries: 10
    depends_on:
      kms:
        condition: service_healthy
    expose:
      - "8020"
      - "9870"

  datanode1:
    image: urykhy/hadoop
    container_name: datanode1
    hostname: datanode1
    domainname: hadoop.docker
    command: /datanode.sh
    depends_on:
      namenode:
        condition: service_healthy
    networks:
      - hadoop
    volumes:
      - hadoop_datanode1:/hadoop/dfs/data
    healthcheck:
      test: test `jps | grep 'DataNode\|NodeManager\|JobHistoryServer' | wc -l` = '3'
      interval: 5s
      timeout: 5s
      retries: 10
    expose:
      - "8042"
      - "9864"
      - "9866"
      - "9867"
      - "19888"

  datanode2:
    image: urykhy/hadoop
    container_name: datanode2
    hostname: datanode2
    domainname: hadoop.docker
    command: /datanode.sh
    depends_on:
      namenode:
        condition: service_healthy
    networks:
      - hadoop
    volumes:
      - hadoop_datanode2:/hadoop/dfs/data
    healthcheck:
      test: test `jps | grep 'DataNode\|NodeManager\|JobHistoryServer' | wc -l` = '3'
      interval: 5s
      timeout: 5s
      retries: 10
    expose:
      - "8042"
      - "9864"
      - "9866"
      - "9867"
      - "19888"

  resourcemanager:
    image: urykhy/hadoop
    container_name: resourcemanager
    hostname: resourcemanager
    domainname: hadoop.docker
    command: yarn resourcemanager
    depends_on:
      namenode:
        condition: service_healthy
    networks:
      - hadoop
    expose:
      - "8088"
    healthcheck:
      test: jps | grep -q ResourceManager
      interval: 5s
      timeout: 5s
      retries: 10

  historyserver:
    image: urykhy/hadoop
    container_name: historyserver
    hostname: historyserver
    domainname: hadoop.docker
    command: yarn timelineserver
    networks:
      - hadoop
    volumes:
      - hadoop_historyserver:/hadoop/yarn/timeline
    expose:
      - "8188"
    healthcheck:
      test: jps | grep -q ApplicationHistoryServer
      interval: 5s
      timeout: 5s
      retries: 10

  nfs:
    image: urykhy/hadoop
    container_name: nfs
    hostname: nfs
    domainname: hadoop.docker
    command: /nfs.sh
    depends_on:
      datanode1:
        condition: service_healthy
      datanode2:
        condition: service_healthy
    networks:
    - hadoop
    expose:
      - "111"
      - "50079"
    profiles: ["nfs"]

  minio:
    image: urykhy/hadoop
    container_name: minio_hdfs
    hostname: minio
    domainname: hadoop.docker
    depends_on:
      datanode1:
        condition: service_healthy
      datanode2:
        condition: service_healthy
    networks:
    - hadoop
    environment:
      MINIO_ACCESS_KEY: minio
      MINIO_SECRET_KEY: minio123
      KRB5USERNAME: minio
      KRB5REALM: KERBEROS.ELF.DARK
      KRB5KEYTAB: /etc/minio.keytab
    command: bash -c "minio gateway hdfs hdfs://namenode.hadoop.docker:8020 --console-address :9009"
    volumes:
      - hadoop_minio:/data
    expose:
      - "9000"
      - "9009"

        #  spark:
        #    image: urykhy/hadoop-spark
        #    container_name: spark
        #    hostname: spark
        #    networks:
        #      - hadoop
        #    environment:
        #      - SPARK_CONF_spark_eventLog_enabled=true
        #      - SPARK_CONF_spark_eventLog_dir=hdfs://namenode.hadoop:8020/spark-logs
        #      - SPARK_CONF_spark_history_fs_logDirectory=hdfs://namenode.hadoop:8020/spark-logs
        #    env_file:
        #      - ./hadoop.env
        #    command: historyserver
        #    depends_on:
        #      - namenode
        #
        #  notebook:
        #    image: urykhy/hadoop-spark-notebook
        #    container_name: notebook
        #    hostname: notebook
        #    networks:
        #      - hadoop
        #    env_file:
        #      - ./hadoop.env
        #    depends_on:
        #      - spark

networks:
  hadoop:
    external: true

volumes:
  hadoop_kms:
    external: true
  hadoop_namenode:
    external: true
  hadoop_datanode1:
    external: true
  hadoop_datanode2:
    external: true
  hadoop_historyserver:
    external: true
  hadoop_minio:
    external: true
